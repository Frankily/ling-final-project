###
# Model Configuration: LLaMA-3 8B
###

base_model: NousResearch/Hermes-2-Theta-Llama-3-70B
sequence_len: 128

# base model weight quantization
load_in_8bit: true

# attention implementation
flash_attention: true

# finetuned adapter config
adapter: lora
lora_model_dir:
lora_r: 4
lora_alpha: 8
lora_dropout: 0.05
lora_target_linear: true
lora_fan_in_fan_out:
lora_modules_to_save: # required when adding new tokens to LLaMA/Mistral
  - embed_tokens
  - lm_head
# for details, see https://github.com/huggingface/peft/issues/334#issuecomment-1561727994

###
# Dataset Configuration: sqlqa
###

datasets:
  - path: data.jsonl
    ds_type: json
    type:
      # Map JSONL fields to the corresponding instruction, input, and output.
      field_instruction: correction
      field_input: input
      field_output: output
      # Custom format to include headers and structure for the chat roles.
      format: "<|begin_of_text|><|im_start|>system\nYou are a grammar error correction writing assistant.<|im_end|>\n<|im_start|>user\nRespond with no framing text. If there are mistakes in the text, fix all mistakes in the text (spelling, punctuation, grammar, etc) and respond with only the correct version of the text free of all mistakes. If there are no errors, respond with the original text. Remember to respond without any framing text in both cases.\nText: {input}<|im_end|>\n<|im_start|>assistant"
        
# dataset formatting config{}
tokens:
  - "<|begin_of_text|>"
  - "<|im_start|>"
  - "<|im_end|>"

special_tokens:
  pad_token: <|end_of_text|>

val_set_size: 0.05

###
# Training Configuration
###

# random seed for better reproducibility
seed: 117

# optimizer config
optimizer: adamw_bnb_8bit
learning_rate: 0.00005
lr_scheduler: cosine
num_epochs: 1
micro_batch_size: 1
gradient_accumulation_steps: 1
warmup_steps: 10

# axolotl saving config
dataset_prepared_path: last_run_prepared
output_dir: ./lora-out

# logging and eval config
logging_steps: 1
eval_steps: 0.05

# training performance optimization config
bf16: auto
tf32: false
gradient_checkpointing: true

###
# Miscellaneous Configuration
###

# when true, prevents over-writing the config from the CLI
strict: false

# "Don't mess with this, it's here for accelerate and torchrun" -- axolotl docs
local_rank:
